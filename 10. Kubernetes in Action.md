
[&laquo; Back](9.%20Getting%20Started%20With%20Kubernetes.md)

# Kubernetes in Action

## Installation

Below are the installation steps for `arm-64` mac-os. They are similar to `linux` and `windows` as well.

Step1: Install `Kubectl` (for more info: [link](https://kubernetes.io/docs/tasks/tools/install-kubectl-macos/))

```bash
brew install kubectl
```

Check if `kubectl` installation

```bash
kubectl version --client
```

Step2: Install `Minicube` for local cluster setup

```bash
brew install minikube
```

```bash
minikube start
```

### How Kubernetes Work?

Kubernetes work with `Objects` 

- Pods
- Deployments
- Services
- Volume
- ‚Ä¶

Objects can be created `Imperatively` and `Declaratively` 

### Pod Object

Pods is the `smallest unit` Kubernetes interacts with

- Contains and runs one or multiple containers
    - The most common use case is `One container per pod` .
- Pods contain shared resources (eg. volumes) for all pod containers
- Has a cluster-internal IP by default
    - Containers inside a Pod can communicate via `localhost`

<aside>
üí°

Pods are designed to be `ephemeral` : Kubernetes will start, stop and replace them as needed.

</aside>

<aside>
üí°

For Pods to be managed for you, you need a `Controller` (e.g. `Deployment` )

</aside>

### Deployment Object

Controls (multiple) Pods

- You set a desired state, Kubernetes then changes the actual state
    - Define which pods and containers to run and the number of instances
- Deployments can be paused, deleted and rolled back
- Deployments can be scaled dynamically (and automatically)
    - You can change the number of desired pods as needed

<aside>
üí°

Deployments manage a Pod for you, you can also create multiple deployments.

</aside>

<aside>
üí°

You therefore typically don‚Äôt control pods, instead you use deployments to set up the desired end state.

</aside>

### Work with a sample example

Build the docker image in the sample project as Containers are the main source for Kubernetes.

Check if `Minikube` is running

- `minikube status`

If its not running, you can start using `minikube start` 

`Kubectl` cli tool is used to send instructions to our `Master` and `Worker` nodes in local development.

<aside>
üí°

We are doing this in imperative approach

</aside>

`kubectl create deployment <name-of-the-deployment> --image=<docker image>` 

- this command creates a deployment

<aside>
üí°

the deployment is running in a separate virtual environment and the docker images which are present in our local machine will not work. So you have to give an image which is present in some 

</aside>

`kubectl get deployments` 

- this command gets all the deployments in the cluster

`kubectl get pods` 

- this returns the pods

`kubectl delete deployment <name-of-the-deployment>` 

- this deletes the deployment

<aside>
üí°

Check the minikube dashboard using `minikube dashboard`

</aside>

### Behind the scenes

![image.png](Assets/Kubernetes%20in%20Action/image.png)

### The ‚ÄúService‚Äù Object

Expose Pods to the Cluster or Externally

- Pods have an internal IP by default - it changes when a Pod is replaced
    - Finding Pods is hard if the IP changes all the time
- Services group Pods with a shared IP
- Services can allow external access to Pods
    - The default (internal only) can be overwritten

<aside>
üí°

Without Services, Pods are very hard to reach and communication is difficult

</aside>

<aside>
üí°

Reaching a Pod from outside the Cluster is not possible at all without Services.

</aside>

### Exposing a Deployment with Service

`kubectl expose deployment <name-of-the-deployment> --port=<port> --type=ClusterIP`  there are different `--type` 

- `--type=`
    - `ClusterIP` It can only be reached inside of the cluster and the IP doesn‚Äôt change.
    - `NodePort` this deployment should expose with the IP of the `Worker Node`
    - `LoadBalancer` will generate unique address and also evenly distributes the requests across worker nodes
        - This type should be present in the deployment resource. In local we are using `minikube` which does support this. If we are deploying in a cloud provider like `AWS` which indeed support LoadBalancer.

`kubectl get services` lists all the services

![Image.png](Assets/Kubernetes%20in%20Action/image%201.png)

The `External-IP` will always be in Pending because we are doing it on local. If we deploy this on a cloud service, the External IP will be present. In local, with minikube to access the external-ip we have to run the below command which returns the external IP

`minikube service <deployment-name>`

To delete service use

`kubectl delete service <deployment-name>`

### Restarting Containers

When the container stops, Kubernetes restarts it

### Scaling in Action

`kubectl scale deployment/<deployment-name> --replicas=3`

with this, we get three Pods and they distribute the load evenly. Even if one Pod crashes, other Pods works as inteneded.

### Updating Deployments

`kubectl set image deployment/<deployment-name> <current-container-name>=<new-image-name>` 

Lets update the above deployment

- `kubectl set image deployment/kube-node-app kub-node-app=jokr123/kub-node-app`

<aside>
üí°

Always add a new tag in the docker build to let Kubernetes update to the latest image. If the tag doesn‚Äôt change, it will not re-update the deployment

</aside>

To check the deployment status

`kubectl rollout status deployment/<deployment-name>`

### Deployment Rollbacks & History

Lets say we are rolling out an image which doesn‚Äôt exist like

`kubectl set image deployment/kube-node-app kube-node-app=jokr123/kube-node-app:3` where this `3 version` tag doesn‚Äôt exist in docker hub. 

In this scenario, Kubernetes tries to start this deployment but fails. Because of its rolling updates, Kubernetes will not `terminate` the old Pod which is running the Old image(which indeed a plus point for us because we used the wrong image)

In this scenario, we can rollback the deployment which is causing problem.

`kubectl rollout undo deployment/<deployment-name>` 

after that check the status

`kubectl rollout status deployment/<deployment-name>`

We can also rollback to older versions of deployments. First check

`kubectl rollout history deployment/<deployment-name>` 

Get more information by adding`--revision=`

`kubectl rollout history deployment/<deployment-name> --revision=<revision-number>`

  

With this `revision` we can rollback to a particular deployment

`kubectl rollout undo deployment/<deployment-name> --to-revision=1` 

## Imperative vs Declarative

Kubernetes allows us to create `A Resource Definition` file which is similar to docker-compose yaml file to add all the configuration settings.

All the above commands is `Imperative` approach where we `run` commands.

With `Declarative` approach, we write the config yaml file and run

- `kubectl apply -f=config.yaml`
- A config file is defined and applied to change the desired state
- Comparable to using `Docker Compose` with compose files

## Creating a Deployment Config File

```yaml
apiVersion: apps/v1 # version 
kind: Deployment # kubernetes object
metadata:
  name: second-app-deployment # name of the deployment
spec: # how this deployment should be configured
  replicas: 1 # number of replicas pods
  selector: # selects the labels to be controlled by the deployment
    matchLabels:
      app: second-app
      tier: backend
  template: # pod object  (template is always a Pod)
    # kind: Pod
    metadata:
      labels:
        app: second-app # key value pairs
        tier: backend
    spec: # pod configuration
      containers:
        - name: second-node
          image: jokr123/kub-node-app:latest
        # - name: ... we can have multiple containers
        #   image: ...

```

## Creating a Service Config File

```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  selector:
    app: second-app
    tier: backend
  ports:
    - protocol: 'TCP'
      port: 80
      targetPort: 8080
    # - protocol: 'TCP'
    #   port: 443
    #   targetPort: 443
  type: LoadBalancer
```

<aside>
üí°

Updating is just changing the configuration and running the Command. Like `kubectl apply -f file.yaml` 

</aside>

<aside>
üí°

To delete resources, use `kubectl delete -f=file.yaml -f=file1.yaml` which deletes the resources not the files.

</aside>

## Merging Deployment and Service Files

```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend
spec:
  selector:
    app: second-app
    tier: backend
  ports:
    - protocol: "TCP"
      port: 80
      targetPort: 8080
    # - protocol: 'TCP'
    #   port: 443
    #   targetPort: 443
  type: LoadBalancer
---
apiVersion: apps/v1 # version
kind: Deployment # kubernetes object
metadata:
  name: second-app-deployment # name of the deployment
spec: # how this deployment should be configured
  replicas: 1 # number of replicas pods
  selector: # selects the labels to be controlled by the deployment
    matchLabels:
      app: second-app
      tier: backend
  template: # pod object  (template is always a Pod)
    # kind: Pod
    metadata:
      labels:
        app: second-app # key value pairs
        tier: backend
    spec: # pod configuration
      containers:
        - name: second-node
          image: jokr123/kub-node-app:latest
        # - name: ... we can have multiple containers
        #   image: ...

```

## More on Labels and Selectors

These are used to connect Pods to Deployments and Pods to Services

`matchLabels` is a modern expression used. In the service.yaml file we did not mentioned this but it‚Äôs same.

If `matchLabels` present, we can also use more powerful selector `matchExpressions` 

```yaml
matchExpressions:
	- {key: app, operator: In ,values: [second-app, first-app]}
```

```yaml
apiVersion: apps/v1 # version 
kind: Deployment # kubernetes object
metadata:
  name: second-app-deployment # name of the deployment
spec: # how this deployment should be configured
  replicas: 1 # number of replicas pods
  selector: # selects the labels to be controlled by the deployment
    # matchLabels:
    #   app: second-app
    #   tier: backend
    matchExpressions:
      - {key: app, operator: In, values: [second-app, first-app]}
  template: # pod object  (template is always a Pod)
    # kind: Pod
    metadata:
      labels:
        app: second-app # key value pairs
        tier: backend
    spec: # pod configuration
      containers:
        - name: second-node
          image: jokr123/kub-node-app:latest
        # - name: ... we can have multiple containers
        #   image: ...

```

### Deleting Resources using Labels

```yaml
apiVersion: v1
kind: Service
metadata:
  name: backend
  labels:
    group: example
spec:
  selector:
    app: second-app
    tier: backend
  ports:
    - protocol: "TCP"
      port: 80
      targetPort: 8080
    # - protocol: 'TCP'
    #   port: 443
    #   targetPort: 443
  type: LoadBalancer
```

```yaml
apiVersion: apps/v1 # version
kind: Deployment # kubernetes object
metadata:
  name: second-app-deployment # name of the deployment
  labels:
    group: example
spec: # how this deployment should be configured
  replicas: 1 # number of replicas pods
  selector: # selects the labels to be controlled by the deployment
    # matchLabels:
    #   app: second-app
    #   tier: backend
    matchExpressions:
      - { key: app, operator: In, values: [second-app, first-app] }
  template: # pod object  (template is always a Pod)
    # kind: Pod
    metadata:
      labels:
        app: second-app # key value pairs
        tier: backend
    spec: # pod configuration
      containers:
        - name: second-node
          image: jokr123/kub-node-app:latest
        # - name: ... we can have multiple containers
        #   image: ...
```

Here we have added `labels` under the `metadata` of both `deployment` and `service` yaml files. Now to delete these resources `impertatively` we can use the below command

- `kubectl delete deployments,services -l group=example` here we are specifying the resources we wanted to delete with specific label defined in the config files

## Liveness Probes

```yaml
apiVersion: apps/v1 # version
kind: Deployment # kubernetes object
metadata:
  name: second-app-deployment # name of the deployment
  labels:
    group: example
spec: # how this deployment should be configured
  replicas: 1 # number of replicas pods
  selector: # selects the labels to be controlled by the deployment
    # matchLabels:
    #   app: second-app
    #   tier: backend
    matchExpressions:
      - { key: app, operator: In, values: [second-app, first-app] }
  template: # pod object  (template is always a Pod)
    # kind: Pod
    metadata:
      labels:
        app: second-app # key value pairs
        tier: backend
    spec: # pod configuration
      containers:
        - name: second-node
          image: jokr123/kub-node-app:latest
          livenessProbe: 
            httpGet:
              path: /
              port: 8080
            periodSeconds: 10
            initialDelaySeconds: 5
        # - name: ... we can have multiple containers
        #   image: ...

```

We can add config to perform health check on the containers using `livenessProbe`

---
[&laquo; Prev: Getting Started With Kubernetes](9.%20Getting%20Started%20With%20Kubernetes.md)
[Next: Managing Data & Volumes with Kubernetes &raquo;](11.%20Managing%20Data%20&%20Volumes%20with%20Kubernetes.md)